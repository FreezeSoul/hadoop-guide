
一、什么是Spark
	1、为什么要学习Spark？讨论MapReduce的缺点不足
		（*）MapReduce的缺点不足：核心Shuffle---> 产生大量I/O
		（*）什么是Spark？http://spark.apache.org/
			 Apache Spark™ is a unified analytics engine for large-scale data processing. 
			
	2、Spark的特点：基于内存
		（1）快
		（2）易用
		（3）通用
		（4）兼容性
		参考官网

二、Spark的体系结构与部署（重点）
	1、体系结构：主从结构（单点故障）
		官网提供了一张图  http://spark.apache.org/docs/latest/cluster-overview.html
	
	2、安装部署
		准备工作：安装Linux、JDK等等
		解压：tar -zxvf spark-2.1.0-bin-hadoop2.7.tgz -C ~/training/
		ps：如果是单独的spark则需要先执行：./build/mvn -DskipTests clean package 编译
		由于Spark的脚本命令和Hadoop有冲突，只设置一个即可（不能同时设置）
		配置文件：/root/training/spark-2.1.0-bin-hadoop2.7/conf/spark-env.sh
	
		（1）伪分布： qujianlei
			spark-env.sh
				export JAVA_HOME=/usr/local/java/jdk1.8.0_161
				export SPARK_MASTER_HOST=qujianlei
				export SPARK_MASTER_PORT=7077
		
			slaves
				qujianlei
		
			启动：sbin/start-all.sh
			Spark Web Console（内置Tomcat：8080） http://ip:8080
				
		（2）全分布：三台
		     Master节点： bigdata112
			 Worker从节点：bigdata113 bigdata114
			spark-env.sh
				export JAVA_HOME=/root/training/jdk1.8.0_144
				export SPARK_MASTER_HOST=bigdata112
				export SPARK_MASTER_PORT=7077		

			slaves
				bigdata112			
				
			复制到从节点上
				scp -r spark-2.1.0-bin-hadoop2.7/ root@bigdata113:/root/training
				scp -r spark-2.1.0-bin-hadoop2.7/ root@bigdata114:/root/training
				
			在主节点上启动: sbin/start-all.sh
		
	3、Spark HA：两种方式 参考讲义
		（1）基于文件目录： 用于开发测试（单机环境）
			（*）将Worker和Application的状态信息写入一个目录
			（*）如果出现崩溃，从该目录进行恢复
			（*）在bigdata111上配置
				(1) 创建一个恢复目录 mkdir /root/training/spark-2.1.0-bin-hadoop2.7/recovery
				(2) 修改配置文件  spark-env.sh
					export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=FILESYSTEM -Dspark.deploy.recoveryDirectory=/root/training/spark-2.1.0-bin-hadoop2.7/recovery"
				
		（2）基于ZooKeeper：用于生产环境
			 前提：搭建ZooKeeper
		     Master节点：  bigdata112 bigdata113
			 Worker从节点：bigdata113 bigdata114		
		
			修改：spark-env.sh
			export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=bigdata001:2181,bigdata002:2181,bigdata003:2181 -Dspark.deploy.zookeeper.dir=/spark"
			
			另外，需要注释掉原来的HOST和PORT那两行注释掉
			
			需要在bigdata113上，单点启动一个master
			   sbin/start-master.sh
			

三、执行Spark Demo程序（bigdata111：伪分布上）
	1、执行Spark任务的工具
		（1）spark-submit: 相当于 hadoop jar 命令 ---> 提交MapReduce任务（jar文件 ）
		                   提交Spark的任务（jar文件 ）
			 Spark提供Example例子：/root/training/spark-2.1.0-bin-hadoop2.7/examples/spark-examples_2.11-2.1.0.jar 
								   java  python  r  resources  scala
								   resources ----> 测试数据(格式：txt json  avro  parquet列式存储文件)  --> Spark SQL中
								   
			示例：蒙特卡罗求PI（3.1415926******）
			bin/spark-submit --master spark://bigdata111:7077 --class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.11-2.1.0.jar 200
						  
			Pi is roughly 3.141484157074208
						

						
		（2）spark-shell：类似Scala的REPL命令行，类似Oracle中的SQL*PLUS
		                  Spark的交互式命令行
						  两种运行模式
						  作为一个Application运行
						  
						  
			（*）本地模式  bin/spark-shell
					不连接到集群，在本地执行任务，类似Storm的本地模式
					日志：
						Spark context Web UI available at http://192.168.157.111:4040
						Spark context available as 'sc' (master = local[*], app id = local-1528291341116).
						Spark session available as 'spark'.
					
					开发程序：*****.setMaster("local")

						
			（*）集群模式 : 连接到集群，在集群执行任务，类似Storm的集群模式
					bin/spark-shell --master spark://bigdata111:7077
					日志：
						Spark context Web UI available at http://192.168.157.111:4040
						Spark context available as 'sc' (master = spark://bigdata111:7077, app id = app-20180606212511-0001).
						Spark session available as 'spark'.
						  
					开发程序：*****
						  
					开发一个WordCount程序：处理HDFS数据
						sc.textFile读取HDFS:  sc.textFile("hdfs://qujianlei:9000/input/data.txt")
						sc.textFile读取本地:  sc.textFile("/root/temp/data.txt")
						  
						程序 
						sc.textFile("hdfs://qujianlei:9000/input/data.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("hdfs://qujianlei:9000/output/0606/spark")
						  
						小技巧：能不能只产生一个分区呢？
						sc.textFile("hdfs://qujianlei:9000/input/data.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).repartition(1).saveAsTextFile("hdfs://qujianlei:9000/output/0606/spark1")  
						
					单步运行WordCount（每一步执行的时候，会产生一个新的RDD集合）
					 var rdd1 = sc.textFile("hdfs://qujianlei:9000/input/data.txt")： 延时读取数据
					 var rdd2 = rdd1.flatMap(_.split(" ")): 将每句话进行分词，再合并到一个集合（Array）
					 var rdd3 = rdd2.map((_,1)) : 每个单词记一次数
					                              完整: rdd2.map(word=>(word,1))
												  
					 var rdd4 = rdd3.reduceByKey(_+_)  把相同的key的value进行累加
					                                   注意：reduceByKey(_+_)  完整: reduceByKey((a,b)=>a+b)  
													   举例：Array((Tom,1),(Tom,2),(Mary,3),(Tom,4))
															第一步：分组
															       (Tom,(1,2,4))
																   (Mary,(3))
																   
															第二步：每组对value求和
															          1+2 = 3
																	  3+4 = 7
					总结：（1）算子（函数、方法）：Transformation延时计算
                                                   Action立即执行
						  （2）RDD之间存在依赖关系：宽依赖、窄依赖











